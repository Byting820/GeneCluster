{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the example PBMC3K dataset\n",
    "\n",
    "   GeneCluster expects raw counts as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 2700 × 32738\n",
       "    var: 'gene_ids'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbmc3k = sc.read_h5ad(\"../data/pbmc3k_raw.h5ad\")\n",
    "pbmc3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../GeneCluster\")\n",
    "from DataProcess import qcFilter,findHVG,min_max_normalization\n",
    "\n",
    "qc_data = qcFilter(pbmc3k)\n",
    "qc_data.index.name = None\n",
    "qc_data.columns.name = None\n",
    "\n",
    "num_HVG = 1000\n",
    "HVG_data = findHVG(qc_data,num_HVG)\n",
    "normal_data = min_max_normalization(HVG_data)\n",
    "normal_data.to_csv(\"../data/normal_data.csv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run GeneCluster on the normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part implements the GeneCluster algoritm\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from tkinter import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score,homogeneity_score\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.metrics import silhouette_score,calinski_harabasz_score,adjusted_rand_score\n",
    "\n",
    "import models\n",
    "import cluster\n",
    "from utils import ExpressionDataset, pseudolabels_assign, UnifLabelSampler, AverageMeter\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # working_dir = os.path.dirname(os.path.abspath('main_copy2.py')) \n",
    "    parser.add_argument('--nmb_cluster', type=int, default=20,\n",
    "                        help='number of cluster for k-means (default: 100)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate (default: 0.05)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                        help='momentum (default: 0.9)')\n",
    "    parser.add_argument('--wd', default=-5, type=float,\n",
    "                        help='weight decay pow (default: -5)')\n",
    "    parser.add_argument('--data_path', metavar='PATH', help='path to dataset',\n",
    "                        default='../data/normal_data.csv')\n",
    "\n",
    "    parser.add_argument('--batch', type=int, default=128,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--workers', type=int, default=16,\n",
    "                        help='number of data loading workers')\n",
    "    parser.add_argument('--epochs', type=int, default=200,\n",
    "                        help='epoch number')\n",
    "    parser.add_argument('--clustering', type=str, choices=['Kmeans'],\n",
    "                        default='Kmeans', help='clustering algorithm (default: Kmeans)')\n",
    "    parser.add_argument('--arch', type=str, choices=['sppCNN'],\n",
    "                        default='sppCNN', help='feature embedding model architecture')\n",
    "    parser.add_argument('--reassign', type=float, default=1.,\n",
    "                        help=\"\"\"how many epochs of training between two consecutive\n",
    "                        reassignments of clusters (default: 1)\"\"\")\n",
    "    parser.add_argument('--ckpt_path', type=str, default='../train_res', help='')\n",
    "    parser.add_argument('--checkpoints', type=int, default=1000,\n",
    "                        help='how many iterations between two checkpoints (default: 1000)')\n",
    "    parser.add_argument('--verbose', '-v', action='store_true',\n",
    "                        default=True, help='verbose mode')\n",
    "    \n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "def compute_features(dataloader, model, N):\n",
    "    end = time.time()\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    model.eval()  # close train model\n",
    "    for batch, (input_tensor, _) in enumerate(dataloader):  \n",
    "        with torch.no_grad():\n",
    "            aux = model(input_tensor).data.cpu().numpy() \n",
    "        \n",
    "        if batch == 0:\n",
    "            features = np.zeros((N, aux.shape[1]), dtype='float32')  \n",
    "\n",
    "        aux = aux.astype('float32')  \n",
    "        if batch < len(dataloader) - 1:  \n",
    "            features[batch * args.batch: (batch + 1) * args.batch] = aux\n",
    "        else:\n",
    "\n",
    "            features[batch * args.batch:] = aux\n",
    "\n",
    "        time_cost = time.time() - end\n",
    "        end = time.time()\n",
    "\n",
    "        if (batch % 200) == 0:\n",
    "            print('{0} / {1}\\t'\n",
    "                  'Time cost: {time:.3f} s'\n",
    "                  .format(batch, len(dataloader), time=time_cost))\n",
    "    return features     \n",
    "    \n",
    "\n",
    "def Visualization(features,data_lists=None): \n",
    "    \"\"\" Cluster visualization \"\"\"\n",
    "   \n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(features)\n",
    "\n",
    "    if data_lists is None:\n",
    "        kmeans = KMeans(n_clusters=args.nmb_cluster,random_state=10)     \n",
    "        kmeans.fit(embedding)\n",
    "        gene_label = kmeans.predict(embedding)\n",
    "        \n",
    "    else:\n",
    "        gene_label = np.zeros(len(features))      \n",
    "        for i in range(len(data_lists)):\n",
    "            gene_label[data_lists[i]] = [i for m in range(len(data_lists[i]))]   \n",
    "\n",
    "    \n",
    "    picture = plt.scatter(embedding[:, 0], embedding[:, 1], s=5, c=gene_label)             \n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "    plt.title('UMAP projection of the dataset')\n",
    "    # plt.gca().legend().remove()\n",
    "    return picture\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    mult-class focalloss,-alpha(1-yi)**gamma *ce_loss(xi,yi)\n",
    "    alpha:class weight.\n",
    "    gamma:Adjusting the rate at which the weights of simple samples decrease.\n",
    "    class_num:class number.\n",
    "    size_average:Loss calculation mode. The default value is the average value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, output, targets):\n",
    "        ce_loss = F.cross_entropy(output, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "\n",
    "def main(args):\n",
    "    # model\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    model = models.__dict__[args.arch](out=args.nmb_cluster)\n",
    "    fea_dim = int(model.top_layer.weight.size()[1])  # torch.Size([100, 200])\n",
    "    model.top_layer = None\n",
    "    # model.features = torch.nn.DataParallel(model.features)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.features.to(device)\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # load data\n",
    "    end = time.time()\n",
    "    dataset = ExpressionDataset(args.data_path)\n",
    "\n",
    "    if args.verbose:\n",
    "        print('Load dataset: {0:.2f} s'.format(time.time() - end))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                            batch_size=args.batch,\n",
    "                                            num_workers=args.workers,\n",
    "                                            pin_memory=True)\n",
    "\n",
    "\n",
    "    deepcluster = cluster.__dict__[args.clustering](args.nmb_cluster) \n",
    "    if args.verbose:\n",
    "            print('Cluster the features with {}'.format(args.clustering))\n",
    "    \n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.SGD(\n",
    "        filter(lambda x: x.requires_grad, model.parameters()),\n",
    "        lr=args.lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=10**args.wd,\n",
    "    )\n",
    "\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    # define loss function\n",
    "    loss_fn = FocalLoss(gamma=2, reduction='mean').cuda()\n",
    "    \n",
    "    # creating checkpoint repo\n",
    "    ckpt_path = os.path.join(args.ckpt_path, 'checkpoints')\n",
    "    if not os.path.isdir(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "    \n",
    "    cluster_log = cluster.Logger(os.path.join(args.ckpt_path,'clusters'))\n",
    "\n",
    "    last_update = 0\n",
    "    patience = 30 \n",
    "    for epoch in range(args.epochs):\n",
    "        end = time.time()\n",
    "        \n",
    "        model.top_layer = None\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1]) \n",
    "        features = compute_features(dataloader, model, len(dataset))\n",
    "        print(features.shape)\n",
    "        np.save(args.ckpt_path+'/features.npy',features)\n",
    "  \n",
    "        clustering_loss = deepcluster.run(features, verbose=args.verbose) \n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            visual = Visualization(features, deepcluster.data_lists)\n",
    "\n",
    "            plt.savefig(os.path.join(args.ckpt_path, 'Epoch{}_cluster_picture.png'.format(epoch)))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        if args.verbose:\n",
    "            print('Assign pseudo labels')            \n",
    "        train_dataset = pseudolabels_assign(deepcluster.data_lists,\n",
    "                                            dataset.datas)\n",
    "\n",
    "        sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),   \n",
    "                                   deepcluster.data_lists)\n",
    "        \n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch,\n",
    "            num_workers=args.workers,\n",
    "            sampler=sampler,\n",
    "            pin_memory=True)\n",
    "        \n",
    "        # set last fully connected layer\n",
    "        mlp = list(model.classifier.children())\n",
    "        mlp.append(nn.ReLU(inplace=True).cuda())  \n",
    "        model.classifier = nn.Sequential(*mlp)\n",
    "        model.top_layer = nn.Linear(fea_dim, len(deepcluster.data_lists))\n",
    "        model.top_layer.weight.data.normal_(0, 0.01)\n",
    "        model.top_layer.bias.data.zero_()\n",
    "        model.top_layer.cuda()\n",
    "\n",
    "         \n",
    "        loss = train(train_dataloader, model, loss_fn, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        silhouette_avg = silhouette_score(features,train_dataset.labels)\n",
    "        ch_score = calinski_harabasz_score(features,train_dataset.labels)\n",
    "        print('silhouette_avg: {:.3f}\\n'\n",
    "              'ch_score: {:.3f}\\n'\n",
    "              .format(silhouette_avg,ch_score))\n",
    "\n",
    "\n",
    "        # print log\n",
    "        if args.verbose:\n",
    "            print('###### Epoch [{0}] ###### \\n'\n",
    "                  'Time: {1:.3f} s\\n'\n",
    "                  'Clustering loss: {2:.3f} \\n'\n",
    "                  'ConvNet loss: {3:.3f}\\n'\n",
    "                  .format(epoch, time.time() - end, clustering_loss, loss))\n",
    "            \n",
    "            try:\n",
    "                NMI = normalized_mutual_info_score(\n",
    "                        cluster.arrange_clustering(deepcluster.data_lists),\n",
    "                        cluster.arrange_clustering(cluster_log.data[-1])\n",
    "                    )\n",
    "                print('NMI against previous assignment: {0:.3f}'.format(NMI))\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            cluster_log.log(deepcluster.data_lists)\n",
    "            #print(cluster_log.data)\n",
    "            print('####################### \\n')\n",
    "            \n",
    "        # save running checkpoint\n",
    "        torch.save({'epoch': epoch + 1,\n",
    "\t\t            'arch': args.arch,\n",
    "                    'clustering': args.clustering,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict()},\n",
    "                   os.path.join(args.ckpt_path, 'checkpoint.pth.tar'))\n",
    "\n",
    "        # early stopping\n",
    "        if epoch == 0:\n",
    "            best_loss = float('inf')\n",
    "        else:\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                last_update = epoch\n",
    "            else:\n",
    "                if epoch - last_update > patience:\n",
    "                    print(\"==============Early stopping!==============\")\n",
    "                    break        \n",
    "\n",
    "\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer, epoch):\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    # create an optimizer for the last fc layer\n",
    "    optimizer_tl = torch.optim.SGD(\n",
    "        model.top_layer.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=10**args.wd,\n",
    "    )\n",
    "\n",
    "    for batch, (input_tensor, target) in enumerate(dataloader):\n",
    "        end = time.time()\n",
    "\n",
    "        # save checkpoint\n",
    "        n = len(dataloader) * epoch + batch\n",
    "        if n % args.checkpoints == 0:\n",
    "            path = os.path.join(\n",
    "                args.ckpt_path,\n",
    "                'checkpoints',\n",
    "                'checkpoint_' + str(n / args.checkpoints) + '.pth.tar',\n",
    "            )\n",
    "            if args.verbose:\n",
    "                print('Save checkpoint at: {0}'.format(path))\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "\t\t        'arch': args.arch,\n",
    "                'clustering': args.clustering,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict()\n",
    "            }, path)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = torch.autograd.Variable(input_tensor.cuda())\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        output = model(input_var)\n",
    "\n",
    "        loss = loss_fn(output, target_var)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), input_tensor.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_tl.zero_grad()\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "        optimizer_tl.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "\n",
    "        if args.verbose and (batch % 200) == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss: {loss.val:.4f} ({loss.avg:.4f})'\n",
    "                  .format(epoch, batch, len(dataloader),\n",
    "                          batch_time=batch_time,\n",
    "                          loss=losses))\n",
    "\n",
    "    return losses.avg\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    args = parse_args()\n",
    "    expr_data = pd.read_csv(args.data_path,sep = '\\t',index_col=0)\n",
    "\n",
    "    if not os.path.isdir(args.ckpt_path):\n",
    "        os.makedirs(args.ckpt_path)\n",
    "    sys.stdout = open(args.ckpt_path + '/result.log', mode = 'w')\n",
    "    main(args)\n",
    "    total_time = (time.time()-start)/60\n",
    "    print('model_all_run_time' + ':' + str(total_time) + 'min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.8_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
